# Analyzer Gateway

O **analyzer-gw** Ã© um gateway HTTP ultra-enxuto que abstrai mÃºltiplos provedores de IA atravÃ©s de uma interface unificada com Server-Sent Events (SSE).

## âœ¨ CaracterÃ­sticas

- **Single Binary**: Zero dependÃªncias, stateless
- **Multi-Provider**: OpenAI, Anthropic, Groq, OpenRouter, Ollama
- **SSE Streaming**: Respostas em tempo real via Server-Sent Events
- **BYOK Seguro**: Bring Your Own Key sem persistÃªncia
- **Usage Metrics**: TransparÃªncia total de tokens, latÃªncia e custos
- **Config YAML**: Trocar providers sem rebuild

## ğŸš€ Quick Start

### 1. Configure providers

Edite `config/config.example.yml`:

```yaml
providers:
  openai:
    type: openai
    base_url: https://api.openai.com
    key_env: OPENAI_API_KEY
    default_model: gpt-4o-mini
```

### 2. Execute o gateway

```bash
# Via Makefile
make run-gw

# Ou diretamente
export OPENAI_API_KEY="your-key-here"
./dist/analyzer-gw
```

### 3. Teste a API

```bash
# Health check
curl http://localhost:8080/healthz

# List providers
curl http://localhost:8080/v1/providers

# Chat com SSE
curl -X POST http://localhost:8080/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "provider": "openai",
    "model": "gpt-4o-mini",
    "messages": [{"role": "user", "content": "Hello!"}],
    "temperature": 0.7
  }'
```

## ğŸ“¡ API Endpoints

### `POST /v1/chat`

Realiza chat completion com streaming SSE.

**Request:**

```json
{
  "provider": "openai",
  "model": "gpt-4o-mini",
  "messages": [
    {"role": "user", "content": "Hello!"}
  ],
  "temperature": 0.7
}
```

**Response (SSE):**

```json
data: {"content": "Hello", "done": false}
data: {"content": "! How", "done": false}
data: {"done": true, "usage": {"tokens": 15, "latency_ms": 1200, "cost_usd": 0.00003}}
```

### `GET /v1/providers`

Lista providers disponÃ­veis e suas configuraÃ§Ãµes.

### `GET /healthz`

Health check endpoint.

## ğŸ” BYOK (Bring Your Own Key)

Envie sua prÃ³pria API key via header:

```bash
curl -X POST http://localhost:8080/v1/chat \
  -H "x-external-api-key: your-key" \
  -H "Content-Type: application/json" \
  -d '{"provider": "openai", "messages": [...]}'
```

## ğŸ“Š Usage Metrics

Cada resposta inclui mÃ©tricas detalhadas:

- **tokens**: Total de tokens utilizados
- **latency_ms**: LatÃªncia da requisiÃ§Ã£o
- **cost_usd**: Custo estimado em USD
- **provider**: Provider utilizado
- **model**: Modelo especÃ­fico

## ğŸ—ï¸ Arquitetura

```plaintext
cmd/gw/main.go              # Bootstrap do gateway
internal/gateway/
â”œâ”€â”€ registry/               # Provider registry e interfaces
â”‚   â”œâ”€â”€ registry.go        # Core registry logic
â”‚   â””â”€â”€ openai.go         # OpenAI provider
â””â”€â”€ transport/             # HTTP transport layer
    â””â”€â”€ http.go           # SSE endpoints
config/config.example.yml       # Provider configuration
```

## ğŸ”§ Build & Deploy

```bash
# Build
make build-gw

# Test
./test_gateway.sh

# Deploy (single binary)
scp dist/analyzer-gw user@server:/usr/local/bin/
```

## ğŸ¯ Por que isso Ã© valioso?

1. **Time-to-Market**: Trocar vendor = editar YAML
2. **TransparÃªncia de Custos**: MÃ©tricas unificadas
3. **Zero Lock-in**: Mesma interface para todos providers
4. **Deploy Simples**: Single binary, stateless
5. **Escalabilidade**: SSE + stateless = horizontal scaling

## ğŸ›£ï¸ Roadmap

- [ ] Anthropic Provider
- [ ] Groq Provider
- [ ] OpenRouter Provider
- [ ] Ollama Provider
- [ ] Rate Limiting
- [ ] Authentication
- [ ] Metrics Dashboard
